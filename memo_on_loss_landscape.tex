\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Pan}
\title{Recent Advances on the Loss Surface of Neural Net}
\begin{document}
\maketitle
\section{Context}
A thorough study of the \textit{geometry} or the \textit{landscape} of the loss function of NN(respecially deep ones) is a critical step towards a further understanding of \textit{the blackbox}. Such a study is also of benefit for the construction of efficient optimization algorithms for deep learning. That is why this topic regains attention in the recent literature of machine learning.
\\
In my latest memo\cite{memo}, I have discussed the possibility of automatically exploiting the one-order geometry of a given loss function for the convenience of applying geometric optimization methods in the context of machine learning. After that, I was aware of the over-generality of my proposed method, which forced me to firstly choose a family of toy loss functions to work with. And this memo is a preliminary context for my future work on my proposed method.   
\\
Actually, the discussion of geometric property of a NN is not a brand new topic. It may be traced back to the introduction of VC-entropy and VC-dimension in statistical learning theory\cite{vapnik2012the} and its application to the study of NN\cite{Anthony2009}. The main idea is to consider the seperating power of a given model on arbitrary distributions. However, these points are too theoretic with a loose relation(at least I have not found out the relation) to what I am studying about. Thus it would not be included in this memo. The ideal style of this memo in my mind should be, as most of the papers I refer to, rich in viewpoints and clear in notations, dealing with some fundamental aspects in machine learning but at the same time, close to problems and phenomena we have met in appplications and experiments.
\section{Outline}
Since this memo is a re-compiled version of half of my paper-based memos over the June, a relatively large number of papers will thus be discussed in this memo. Although the topic is unique, \textbf{the landscape of loss function of neural net}, both the techniques adopted and the viewpoints are however miscellaneous. An outline may be of help here.

In Section 3, I will discuss the core formulation of \textit{natural gradient descent} introduced by \textit{Amari} in 1998, which is in essence a  \textit{steepest gradient descent} over the parameter manifold equipped with the corresponding \textit{Fisher metric}. The method has been applied to a simple neural net with one hidden layer in the original paper. Although we will no sooner find out that natural gradient descent is general \and theoretically solid but hard to be generalized to deep neural net in a computational efficient way, it is still be considered as a significant breakthrough of geometrical methods applied in NN.

In Section 4, discussion would be concentrated on several specific constraints imposed on the search space that are recently reported\cite{Arjovsky2016} to be both efficient together with better performance of the same recurrent neural net. The underlying mechanism has not been revealed(since either the imposed \textit{orthogonality} or the \textit{unitarity} seems to come from nothing) and I am extremely curious about it. 

In Section 5, a series of work\cite{Neyshabur201503}\cite{Neyshabur201506} on exploiting the \textit{symmmetry-invariance} in neural net will be discussed. It seemingly lies in the opposite polarity of \textit{natural gradient descent} since it tries to introduce a manifold structure via a quite subtle observation of a hiding equivalent relation in the paremter space. The method is at last crystallized into a so-called \textit{group regularization term}. This paper starts from some experimental observations but comes to a rather general solution, which impresses me a lot.

In Section 6, we end the memo by a series of work on depicting the loss surface of neural net in a general setting.    



 



\begin{thebibliography}{9}
\bibitem{memo} Pan, \textit{Learning Natural Descent via Implicit Approximation of Underlying Parameter Manifold}
\bibitem{vapnik2012the} Vapnik V. \textit{The Nature of Statistical Learning Theory}[J]. Technometrics, 2012, 38(4): 409-409.
\bibitem{Anthony2009}Anthony, Martin, and Peter L. Bartlett. \textit{Neural network learning: Theoretical foundations.} cambridge university press, 2009.
\bibitem{Amari1998}Amari S. \textit{Natural gradient works efficiently in learning}[J]. Neural Computation, 1998, 10(2): 251-276.
\bibitem{Arjovsky2016}Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \textit{Unitary evolution recurrent neural networks.} International Conference on Machine Learning. 2016.
\bibitem{Neyshabur201503}Neyshabur, B., Tomioka, R., \and Srebro, N. (2015). Norm-Based Capacity Control in Neural Networks. Retrieved from http://arxiv.org/abs/1503.00036
\bibitem{Neyshabur201506}Neyshabur, B., Salakhutdinov, R., \and Srebro, N. (2015). Path-SGD: Path-Normalized Optimization in Deep Neural Networks, 1â€“12. Retrieved from http://arxiv.org/abs/1506.02617
\end{thebibliography}
\end{document}